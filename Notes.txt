Notes to self:
1- Scraping is done through URL
	Ex: https://www.workplacerelations.ie/en/search/?decisions=1&q=%22Labour%22&from=4/8/2025&to=1/12/2025&body=1,3,15376
2- Ids Determined for body:
	Employment Appeals Tribunal: 2
	Equality Tribunal: 1
	Labour Count Id: 3
	Workplace Relations Commission: 15376
3- q = query to search (surround with double quotes)
3- decisions? 
4- Async requests as per documentation? 

Structure:
1- Downloaded and set up Git
2- Create Repo: ScraperAssessment
3- Cloned and connected to VSCode to GitHub
4- Created venv for this project
5- Downloaded the two libraries needed: scrapy and pymongo
6- Created a MongoDB Cluster (free version)
	6.1 Allowed access for all
	6.2 Username: admin
	6.3 Password: admin
	6.4 Connection String: mongodb+srv://admin:<admin>@scrapercluster.k435ykh.mongodb.net/?appName=ScraperCluster
Scraper:
1- Scraper is ran through a command outside the python editor. 
2- Spider → yields Request → Scheduler → Downloader → Response → callback
3- Start project for scraper (everytime?)
  - scrapy startproject projectName
  - Create Spider within ProjectName/scraper/spiders
4- The request with relative path will be saved in scraper directory (created by engine)

to call the scraper:
	scrapy crawl documents -a start_date=04/11/2025  -a end_date=14/11/2025 -a query=labour -a body="Labour Count,Workplace Relations 	Commission" -a partition=7


Code:
6-  
